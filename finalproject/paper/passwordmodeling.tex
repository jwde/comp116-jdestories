%%
%% This is LaTeX2e input.
%%

%% The following tells LaTeX that we are using the 
%% style file amsart.cls (That is the AMS article style
%%
\documentclass{amsart}

\usepackage{listings}
\usepackage{color}
\usepackage{bbm}

\renewcommand\lstlistingname{Quelltext}

\lstset{
    language=Python,
    basicstyle=\small\sffamily,
    numbers=left,
    numberstyle=\tiny,
    frame=tb,
    tabsize=4,
    columns=fixed,
    showstringspaces=false,
    showtabs=false,
    keepspaces,
    commentstyle=\color{red},
    keywordstyle=\color{blue}
}

%% This has a default type size 10pt.  Other options are 11pt and 12pt
%% This are set by replacing the command above by
%% \documentclass[11pt]{amsart}
%%
%% or
%%
%% \documentclass[12pt]{amsart}
%%

%%
%% Some mathematical symbols are not included in the basic LaTeX
%% package.  Uncommenting the following makes more commands
%% available. 
%%

%\usepackage{amssymb}

%%
%% The following is commands are used for importing various types of
%% grapics.
%% 

%\usepackage{epsfig}  		% For postscript
%\usepackage{epic,eepic}       % For epic and eepic output from xfig

%%
%% The following is very useful in keeping track of labels while
%% writing.  The variant   \usepackage[notcite]{showkeys}
%% does not show the labels on the \cite commands.
%% 

%\usepackageshowkeys}


%%%%
%%%% The next few commands set up the theorem type environments.
%%%% Here they are set up to be numbered section.number, but this can
%%%% be changed.
%%%%

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}


%%
%% If some other type is need, say conjectures, then it is constructed
%% by editing and uncommenting the following.
%%

%\newtheorem{conj}[thm]{Conjecture} 


%%% 
%%% The following gives definition type environments (which only differ
%%% from theorem type invironmants in the choices of fonts).  The
%%% numbering is still tied to the theorem counter.
%%% 

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{example}[thm]{Example}

%%
%% Again more of these can be added by uncommenting and editing the
%% following. 
%%

%\newtheorem{note}[thm]{Note}


%%% 
%%% The following gives remark type environments (which only differ
%%% from theorem type invironmants in the choices of fonts).  The
%%% numbering is still tied to the theorem counter.
%%% 


\theoremstyle{remark}

\newtheorem{remark}[thm]{Remark}


%%%
%%% The following, if uncommented, numbers equations within sections.
%%% 

\numberwithin{equation}{section}


%%%
%%% The following show how to make definition (also called macros or
%%% abbreviations).  For example to use get a bold face R for use to
%%% name the real numbers the command is \mathbf{R}.  To save typing we
%%% can abbreviate as

\newcommand{\R}{\mathbf{R}}  % The real numbers.

%%
%% The comment after the defintion is not required, but if you are
%% working with someone they will likely thank you for explaining your
%% definition.  
%%
%% Now add you own definitions:
%%

%%%
%%% Mathematical operators (things like sin and cos which are used as
%%% functions and have slightly different spacing when typeset than
%%% variables are defined as follows:
%%%

\DeclareMathOperator{\dist}{dist} % The distance.



%%
%% This is the end of the preamble.
%% 


\begin{document}

%%
%% The title of the paper goes here.  Edit to your title.
%%

%%\title{Probabilistic Password Modeling: Applications of Machine Learning in Predicting Passwords}
\title{Probabilistic Password Modeling: Predicting Passwords with Machine Learning}

%%
%% Now edit the following to give your name and address:
%% 

%\author{Jay DeStories}
%\email{jaydestories@gmail.com}
%\urladdr{www.jwde.github.io} % Delete if not wanted.

%%
%% If there is another author uncomment and edit the following.
%%

%\author{Second Author}
%\address{Department of Mathematics, University of South Carolina,
%Columbia, SC 29208}
%\email{second@math.sc.edu}
%\urladdr{www.math.sc.edu/$\sim$second}

%%
%% If there are three of more authors they are added in the obvious
%% way. 
%%

%%%
%%% The following is for the abstract.  The abstract is optional and
%%% if not used just delete, or comment out, the following.
%%%

%\begin{abstract}
%Natural Language Processing Problem Set 2
%\end{abstract}

%%
%%  LaTeX will not make the title for the paper unless told to do so.
%%  This is done by uncommenting the following.
%%

\maketitle

\begin{center}
\small{JAY DESTORIES}\\
\small{MENTOR: ELIF YAMANGIL}\\[8ex]
\end{center}

%%
%% LaTeX can automatically make a table of contents.  This is done by
%% uncommenting the following:
%%

%\tableofcontents

%%
%%  To enter text is easy.  Just type it.  A blank line starts a new
%%  paragraph. 
%%



%%
%%  To put mathematics in a line it is put between dollor signs.  That
%%  is $(x+y)^2=x^2+2xy+y^2$
%%

%%
%%% Displayed mathematics is put between double dollar signs.  
%%


%%
%% A Theorem is stated by
%%

%\begin{thm} The square of any real number is non-negative.
%\end{thm}

%%
%% Its proof is set off by
%% 


%%
%% A new section is started as follows:
%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}\textbf{Abstract}\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Many systems use passwords as the primary means of authentication. As the length of a password grows, the search space of possible passwords grows exponentially. Despite this, people often fail to create unpredictable passwords. This paper will explore the problem of creating a probabilistic model for describing the distribution of passwords among the set of strings. This will help us gain insight into the relative strength of passwords as well as alternatives to existing methods of password candidate generation for password recovery tools like John the Ripper. This paper will consider methods from the field of natural language processing and evaluate their efficacy in modeling human-generated passwords.

\newpage

\tableofcontents

\newpage

\section{Introduction}
Passwords are commonly used by the general public for authentication with computer services like social networks and online banking. The purpose of a password, and authentication in general in the scope of computer systems, is to confirm an identity. For instance, when using an online banking service you provide a password so that you can perform actions on an account, but others cannot. This sense of security relies on a password being something that you can remember, but others cannot discover. The security of a system protected by a password is dependent on that password being difficult to guess. This is helped by the fact that an increase in the length of a password exponentially increases the number of possible passwords. Despite this, people often create unoriginal and easy to guess passwords. In a 2014 leak of approximately 5 million gmail account passwords, 47779 users had used the password "123456"\footnote{Password corpora referenced are available at: http://tinyurl.com/pwdcorpora}. In fact, there are so many instances of different people using the same passwords that one of the dominant ways to guess passwords in password recovery tools like John the Ripper is to simply guess every password in a list of leaked passwords. It is apparent that there is some underlying thought process in coming up with a password that leads to people creating similar or even identical passwords. This paper will attempt to find that structure in the form of a probabilistic model. It will consider some techniques from the field of natural language processing for inferring the structure of the language. If there is a good way to model the language of passwords, it could be used to generate new password guesses that don't already exist in a wordlist, improving the performance of password recovery tools when wordlists are exhausted. It could also evaluate the strength of a password in terms of the expected number of guesses to find it given a model.

\section{To the Community}
This paper deals mainly with the technical aspects of guessing passwords. The purpose of this paper is to explore the weaknesses of using passwords for authentication. This is an interesting problem because good passwords by definition do not fit well into a predictive model (a good password is hard to guess). We will consider ways of guessing which passwords a person might use and evaluating the strength of a password, with the intention not only of demonstrating these techniques, but also of encouraging the use of stronger passwords and additional security measures like two-factor authentication.

%\section{Classifying the Language of Passwords}
%\subsection{Regular Languages}
%\subsection{Context-Free Languages}
%\subsection{Context-Sensitive and Recursively Enumerable Languages}

\section{Probability Modeling}
\subsection{N-Gram Language Model}
N-grams make sequence modeling practical by making a Markov assumption: the probability of an element given the previous elements is approximately equal to the probability of that element given the previous $n$ elements. For example, an n-gram of order $2$ (bigram) assumes that $P(e_j \mid e_1, ..., e_{j - 1}) \approx P(e_j \mid e_{j - 1})$. Then, by the chain rule the probability of a sequence
$$P(e_1, ..., e_n) = P(e_1)P(e_2 \mid e_1)P(e_3 \mid e_1, e_2)...P(e_n \mid e_1, ..., e_{n - 1})$$
$$\approx P(e_1)P(e_2 \mid e_1)P(e_3 \mid e_2)...P(e_n \mid e_{n - 1})$$
Using transition probabilities like $P(e_2 \mid e_1), P(e_3 \mid e_2)$ rather that considering all previous elements in the sequence makes it easier to extrapolate to other arbitrary sequences. Consider the password "123456789" in the training corpus for a bigram model describing passwords. Using a bigram captures the idea that numeric character sequences are likely to appear in numeric order and would assign high probabilities to sequences like "123". A bigram model consists of the matrix of transition probabilities from all tokens to all other tokens. To train a bigram model on the training corpus, we want to Maximum likelihood estimation (MLE)-- the matrix which maximizes the probability of the training corpus.
$$\theta_{kj} = P(v_j \mid v_k) : 0 \leq \theta_{kj} \leq 1, \Sigma_j \theta{kj} = 1 \forall k$$
Then the maximum likelihood estimation for $\theta_{kj}$, $\hat{\theta}_{kj}$ is:
$$\hat{\theta}_{kj} = argmax\{P(c_1, ..., c_n)\}$$
Log is monotonic, so we can we can rewrite this as:
$$\hat{\theta}_{kj} = argmax\{log P(c_1, ..., c_n)\}$$
Then by our bigram assumption:
$$\hat{\theta}_{kj}= argmax\{log \Pi_i P(c_1 \mid c_{i - 1})\}$$
$$= argmax\{log \Pi_i \Pi_{kj} \theta_{kj}^{\mathbbm{1}(c_{i - 1} = v_k \land c_i = v_j)}\}$$
$$= argmax\{log \Pi_{kj} \theta_{kj}^{\Sigma_i \mathbbm{1}(c_{i - 1} = v_k \land c_i = v_j)}\}$$
$$= argmax\{log \Pi_{kj} \theta_{kj}^{n_{kj}}\}$$
$$= argmax\{\Sigma_{kj} n_{kj} log \theta_{kj}\}$$
Apply Lagrange multiplier, differentiate and set equal to zero to find the maximum subject to our constraints:
$$\frac{\partial}{\partial \theta_{kj}}[\Sigma_k \Sigma_j n_{kj} log \theta_{kj} - \Sigma_k \lambda_k (\Sigma_j \theta_{kj} - 1)] = 0$$
$$\frac{n_{kj}}{\hat{\theta}_{kj}} - \lambda_k = 0$$
$$\frac{n_{kj}}{\hat{\theta}_{kj}} = \lambda_k$$
$$\hat{\theta}_{kj} = \frac{n_{kj}}{\lambda_k}$$
$$\hat{\theta}_{kj} = \frac{n_{kj}}{\Sigma_j n_{kj}}$$
So to get the MLE estimate $\hat{\theta}_{kj}$, add up all transitions from $k$ to $j$ and divide that by the number of transitions from $k$ to anything.

For passwords, consider all transitions in every sequence in the training corpus, including tokens for the start and end of each password. Count the number of times each character appears at the start of a transition, and the number of times each transition appears. The probability of bigram $P(B \mid A)$ is equal to the number of transitions from A to B divided by the number of transitions starting at A. See the appendix for implementation specific details.

\subsection{Hidden Markov Model}
Hidden Markov models (HMMs) are similar to n-grams in that they model sequences based on the Markov assumption that we can approximate the likelihood of future events by considering only recent ones. In modeling sequences with n-grams, we considered probabilities of transitions between observable events. In the case of password modeling, those events are the observable characters of a password. In HMMs, we instead consider an unobservable sequence of events which influence the observable events. In the context of natural language processing, one example of applications of Hidden Markov models is in part-of-speech tagging. In that case, the hidden sequence is a sequence of parts of speech, each of which relates to a word. An HMM defines the probability of n-grams of the hidden sequence, as well as the probability of an observation at any given hidden state. To train an HMM for a problem like part-of-speech tagging, humans would manually map parts of speech to words in a training corpus, then use computers to count the number of each n-gram $C_{s_1,...,s_n,b}$, the compute probabilities of each n-gram 
$$P(s_1,...,s_n,b) = \frac{C_{s_1,...,s_n,b}}{\Sigma_a C_{s_1,...,s_n,a}}$$
Computing the emission probabilities (probability of an observed event given a hidden state), we would just count up the number of times each observation appears for a particular hidden state, then divide that by the number of times the hidden state appears. This is a supervised training approach. However, using Hidden Markov models to describe passwords is less intuitive. There is no established way to tag the characters in a password, so there are no labeled training corpora. This means that short of coming up with a system for tagging characters and manually tagging the dataset for this purpose, the only option to train an HMM for password modeling is unsupervised training. This begs the question: why should we use Hidden Markov models for this task? Despite no established tagging system, some patterns come to mind when looking at password lists. Some people create passwords based on words or other common sequences with certain characters substituted. A common example of this is interchangably using "e", "E", and "3", "a", "A", and "4", "l", "L", and "1", etc. An HMM could model this relationship well by grouping characters which are commonly substituted for one another into common hidden states, then modeling their relative popularity as an emission probability from that hidden state. Another common pattern in passwords is the inclusion of a birth year in the password. Consider passwords that follow the pattern "word/common phrase" + "birth year". An HMM may be able to tag the digits in a birth year as individual hidden states. This could let it capture aspects of the data like the fact that numerically higher digits occur more frequently in the tens place for a birth year. This being said, we don't know if the HMM will be able to learn these types of specific details. We will train an HMM on passwords using the Baum Welch algorithm. This approach will use a fixed number of hidden states, randomly initialize our transition and emission probabilities, then use Expectation Maximization to approach a local maximum for the likelihood of the training corpus. Using NLTK's implementation of an HMM will simplify our code for the purposes of this paper. For practical applications, it may be useful to reimplement and optimize our models.

%%\subsection{Context-Free Grammar}

\section{Generating Passwords}
For each model, we will consider a way of sampling passwords one at a time. We can then evaluate the performance of a model based on how many unique passwords which exist in the test corpus it generates given some fixed number of attempts. For the training corpus, we will use a list of 14,344,391 passwords exposed in a Rockyou leak. For the test corpus, we will use all but the last 5,000 passwords from a list of 3,135,920 passwords exposed in a Gmail leak. We will keep the last 5,000 passwords from the Gmail leak as a held-out corpus\footnote{Password corpora referenced are available at: http://tinyurl.com/pwdcorpora}.
\subsection{Baseline}
The baseline is based on the default wordlist attack in John the Ripper, a popular password recovery tool. It will yield every password in the training corpus, in order. Once it has exhausted the training corpus, it will incrementally try all passwords given a vocabulary of characters.
\subsection{Bigram Language Model}
To generate a password from the bigram model, we will set our current character to the start token, then repeatedly do a weighted random selection of a next character given the transition probabilities until transitioning to the end token.
\subsection{Hidden Markov Model}
Random sampling from an HMM is similar to sampling from an n-gram, except instead of sampling transitions between observations, we sample transitions between hidden states, then for each step also sample an emission for that hidden state. For the purposes of this paper, we can use NLTK's implementation of an HMM, which provides a method to perform random sampling.
%%\subsection{Context-Free Grammar}

\subsection{Results}
\begin{center}
\begin{tabular}{||c c c c||}
\hline
Model & Guesses & Training Passwords Used & Correct Guesses
\\ [0.5ex]
\hline\hline
Baseline & 100,000 & 10 & 4,612 \\
\hline
Baseline & 100,000 & 100 & 6,278 \\
\hline
Baseline & 100,000 & 1,000 & 6,011 \\
\hline
Baseline & 100,000 & 14,344,391 & 81,494 \\
\hline
Bigram LM & 100,000 & 10 & 45 \\
\hline
Bigram LM & 100,000 & 100 & 2,540 \\
\hline
Bigram LM & 100,000 & 1,000 & 4,215 \\
\hline
Bigram LM & 100,000 & 14,344,391 & 3,985 \\
\hline
HMM (10 hidden states) & 100,000 & 10 & 315 \\
\hline
HMM (10 hidden states) & 100,000 & 100 & 1,531 \\
\hline
HMM (10 hidden states) & 100,000 & 1,000 & 1,760 \\
\hline
HMM (50 hidden states) & 100,000 & 10 & 99 \\
\hline
HMM (50 hidden states) & 100,000 & 100 & 1,795 \\
\hline
HMM (50 hidden states) & 100,000 & 1,000 & 2,798 \\
\hline
HMM (100 hidden states) & 100,000 & 10 & 80 \\
\hline
HMM (100 hidden states) & 100,000 & 100 & 1,430 \\
\hline
HMM (100 hidden states) & 100,000 & 1,000 & 2,913 \\
[1ex]
\hline
\end{tabular}
\end{center}

This paper will not consider the efficacy of HMMs trained with more than $1000$ passwords, since training time gets too long. This is worth considering as a drawback to using unsupervised learning for password predictor models.\\
Since the baseline is based on the behavior of a popular password recovery tool, it is unsurprising that it outperforms the language models. The results show that the best precision in a language model was from the bigram model trained on the first $1000$ passwords from the training corpus. This doesn't necessarily mean that the HMM shouldn't be used. Since the bigram model and the HMM model the language differently, it is possible that each may be likely to guess passwords that the other could not. Most encouraging is that all of the models trained on $1000$ or fewer inputs were able to guess more than $1000$ passwords given $100000$ tries. This demonstrates that they have the capability to extrapolate from the input, making them potentially useful additions to the baseline for password recovery. Since the strength of a password is based on how difficult it is to guess, this means that these models are useful for evaluating password strength. If we can expect them to guess a given password more quickly than the baseline, then that password is weaker than we may have otherwise thought.

\section{Applications}
\subsection{Password Strength}
If we conclude that these models are good at guessing passwords, then we could estimate the strength of a password by the expected number of guesses the models would need to make to guess it. We care most about the worst case scenario, so we will say that strength relates to the minimum expected number of guesses from a model.\\
Given a password $password$ and a model $M$ which evaluates $P(password \mid M) = p$, define the Bernoulli random variable $C$ as $1$ if we guess $password$ and $0$ if we do not guess $password$ in some trial. The expectation $E(C)$ for a Bernoulli random variable is equal to its probability, $p$. Therefore the expected number of guesses to find $password$ is $n$, satisfying:\\
$$1 = E(\Sigma_{k = 1}^n C_k)$$
For $E(C) = E(C_1) = ... = E(C_n)$
$$E(\Sigma_{k = 1}^n C_k) = \Sigma_{k = 1}^n E(C_k) = nE(C_k) = 1$$
$$\implies n = \frac{1}{E(C_k)} = \frac{1}{p}$$
But $p$ is, in many cases, small enough to cause floating point imprecision, so we deal with $log(p)$ in implementation.
$$n = \frac{1}{exp(log(p))} = exp(-log(p))$$
For the baseline, the order of passwords guesses is known so the guess number for any particular password is known.\\
See the appendix for a password strength estimate implementation.\\

\subsection{Password Recovery}
Password recovery tools function by guessing passwords, one at a time, until one is correct. In most cases, this means taking a hashed password and hashing password guesses until the hashes match. The baseline used in this paper is based on the wordlist attack behavior for John the Ripper, a popular password recovery tool. The models discussed in this paper for guessing passwords could easily be applied in password recovery tools. For recovering passwords based on a hash, we would just need to implement the hashing algorithm, then hash each guess and compare it against the original password hash.
These models would be useful as additional techniques for guessing passwords if they are likely to guess some probable passwords that existing tools are unlikely to guess. They are therefore potentially useful primarily for guessing passwords which do not occur in wordlists.
When given training sets of 10, 100, and 1000 passwords, both the bigram languge model and the HMM are able to extrapolate, correctly guessing many more unique passwords than they were given. This is encouraging. However, in all cases the baseline outperforms these models. This does not mean that the models are not useful, just that they should not replace the baseline technique, but rather be used in parallel.

\section{Future Work}
Further work in modeling passwords could involve both improving the models described in this paper as well as building new ones. It would be interesting to try modeling passwords with trigrams, four-grams, etc., or with a context-free grammar. An obvious weakness of these models in generating passwords is that there is nothing stopping them from generating the same passwords repeatedly. If we could prune the set of possible generated passwords, it could improve performance by preventing this type of redundancy. If we store every generated password and exclude it from being generated again, the bigram language model's correct guesses goes up from 3,985 to 4,771 when trained on the full training corpus. This exact approach isn't practical because it requires storing every generated password in memory, but it demonstrates the potential improvement with some sort of pruning. Another apparent weakness in this paper is the lack of sufficient computational resources to properly test all models. The forward backward algorithm used to train the HMM takes too long on consumer grade hardware to test it with large training sets. However, many people or organizations who would be interested in guessing passwords have access to much more powerful hardware, and training could be further sped up with a better optimized implementation, for example by taking advantage of graphics processors to parallelize the task. Therefore, it would be interesting to see how the HMM performs with much larger training sets.\\
This paper considered the precision of models at guessing passwords. It would also be interesting to know the recall-- what portion of the test corpus are we able to guess. This is harder to test and is less relevant when computational resources are scarce. This is similar to the problem is estimating password strength, in that it considers the likelihood of a password being guessed.

\section{Conclusion}
We've shown that using language models to guess passwords is effective enough to encourage further exploration. Although none could beat the precision of traditional password recovery techniques, all models were able to extrapolate based on an input set of passwords to a set containing a larger number of correct passwords.\\
This paper outlines one method for evaluating the strength of a password. That being said, it provides no guarrantees for the strength of a password. It is most effective at recognizing passwords that are weak.\\
The important take-away from this paper is how effectively we can guess passwords, even with just traditional methods. Given how much we rely on passwords for critical security, it is important to pick passwords which are harder to guess, but also to keep in mind that regardless of how long or seemingly complex some passwords to be, they may still be vulnerable. Therefore it's important not to rely solely on passwords for security. Other security measures like two-factor authentication improve our chances at keeping systems secure.\\

\newpage

\begin{thebibliography}{9}
\bibitem{latexcompanion}
Bolch, G.; Greiner, S.; Meer, H.; Trivedi, K. (2006) Queueing Networks and Markov Chains
\textit{John Wiley \& Sons, Inc.}

\bibitem{latexcompanion}
Ghahramani, Z. (2001) An Introduction to Hidden Markov Models and Bayesian Networks.
\textit{International Journal of Pattern Recognition and Artificial Intelligence.}

\bibitem{latexcompanion}
Rabiner, L.R., \& Juang, B.H. (1986) An Introduction to Hidden Markov Models.
\textit{IEEE ASSP Magazine}

\bibitem{latexcompanion}
Rosenfeld, Ronald. Two Decades of Statistical Language Modeling: Where do we go from here?
www.cs.cmu.edu/\textasciitilde{}roni/papers/survey-slm-IEEE-PROC-0004.pdf

\bibitem{latexcompanion}
Seneta, E. (1981) Non-negative matrices and Markov chains. 2nd rev. ed.
\textit{Springer Series in Statistics}

\end{thebibliography}

\newpage

\section{Appendix}
\subsection{Language Model Evaluation}
\begin{lstlisting}
"""
    Test the performance of various password models
"""

import lmgenerator
import hmmgenerator

# basic wordlist attack
def baselineGenerator(training_corpus):
    for pwd in training_corpus:
        yield pwd
    vocabulary = set()
    for pwd in training_corpus:
        for c in pwd:
            vocabulary.update([c])
    vocabulary = list(vocabulary)
    def bruteForce(vocabulary, length):
        if length == 1:
            for c in vocabulary:
                yield c
        else:
            for c in vocabulary:
                for password_end in bruteForce(vocabulary, length - 1):
                    yield c + password_end
    length = 1
    while True:
        for pwd in bruteForce(vocabulary, length):
            yield pwd
        length += 1

def passwordStrength(password, training_corpus, bigramlm, hmm):
    expectations = []
    # baseline
    baseline_guess = 0
    if password in training_corpus:
        baseline_guess = training_corpus.index(password) + 1
    else:
        vocabulary = set()
        for pwd in training_corpus:
            for c in pwd:
                vocabulary.update([c])
        vocabulary = list(vocabulary)
        guessable = True
        for c in password:
            if c not in vocabulary:
                guessable = False
        if not guessable:
            baseline_guess = float('inf')
        else:
            baseline_guess = len(training_corpus)
            for pwd_len in xrange(1, len(password)):
                baseline_guess += pow(len(vocabulary), pwd_len)
            def helper(pwd):
                if len(pwd) == 1:
                    return vocabulary.index(pwd[0]) + 1
                return pow(len(vocabulary), len(pwd) - 1) * vocabulary.index(pwd[0]) + \
                       helper(pwd[1:])
            baseline_guess += helper(password)
    expectations.append(baseline_guess)

    if bigramlm:
        expectations.append(bigramlm.ExpectedGuesses(password))
    if hmm:
        expectations.append(hmm.ExpectedGuesses(password))
    return min(expectations)

# See how many things in test_corpus the generator can guess with some number of
# tries
def testGenerator(gen, test_corpus, tries):
    found = 0
    test_set = set(test_corpus)
    guesses = set()
    for i in xrange(tries):
        guess = gen.next()
        if not guess in guesses:
            guesses.update([guess])
            if guess in test_set:
                found += 1
    return found

def testCorpora(training_corpus, test_corpus):
    print "First 5 training passwords: ", training_corpus[:5]
    print "First 5 test passwords: ", test_corpus[:5]

    tries = 100000
    baseline = testGenerator(baselineGenerator(training_corpus), test_corpus, tries)
    print "Baseline wordlist attack -- %d tries: %d." % (tries, baseline)
    bigramlm = lmgenerator.BigramLM(training_corpus)
    bigramlmgen = bigramlm.Generator()
    bigramlmresults = testGenerator(bigramlmgen, test_corpus, tries)
    print "Bigram LM attack -- %d tries: %d." % (tries, bigramlmresults)
    hmmlm = hmmgenerator.HMMLM(training_corpus, 100)
    hmmgen = hmmlm.Generator()
    hmmresults = testGenerator(hmmgen, test_corpus, tries)
    print "HMM attack -- %d tries: %d." % (tries, hmmresults)
    print "Password strength test:"
    print "123456:", passwordStrength("123456", training_corpus, bigramlm, hmmlm)
    print "123456789123:", passwordStrength("123456789123", training_corpus, bigramlm, hmmlm)
    print "mingchow:", passwordStrength("mingchow", training_corpus, bigramlm, hmmlm)
    print "0a0I8DV:", passwordStrength("0a0I8DV", training_corpus, bigramlm, hmmlm)
    print "AtiK0nAOLP3y:", passwordStrength("AtiK0nAOLP3y", training_corpus, bigramlm, hmmlm)
    print "correcthorsebatterystaple:", passwordStrength("correcthorsebatterystaple", training_corpus, bigramlm, hmmlm)

 

def main():
    print "################################################################"
    print "Training corpus: rockyou"
    print "Test corpus: gmail"
    print "################################################################"
    rockyou_nocount = open('corpora/rockyou_nocount', 'r')
    training_corpus = [pwd.rstrip() for pwd in rockyou_nocount][:10]
    gmail_nocount = open('corpora/gmail_nocount', 'r')
    gmail_corpus = [pwd.rstrip() for pwd in gmail_nocount]
    test_corpus = gmail_corpus[:-5000]
    held_out_corpus = gmail_corpus[-5000:]
    testCorpora(training_corpus, test_corpus)


if __name__ == "__main__":
    main()
\end{lstlisting}
\newpage
\subsection{Bigram Language Model Implementation}
\begin{lstlisting}
from math import log, exp, isnan
import random
from decimal import *

start_token = "<S>"
end_token = "</S>"

def Preprocess(corpus):
    return [[start_token] + [token for token in pwd] + [end_token] for pwd in corpus]

class BigramLM:
    def __init__(self, training_corpus):
        self.bigram_counts = {}
        self.unigram_counts = {}
        self.Train(training_corpus)

    def Train(self, training_corpus):
        training_set = Preprocess(training_corpus)
        for pwd in training_set:
            for i in xrange(len(pwd) - 1):
                token = pwd[i]
                next_token = pwd[i + 1]
                if not token in self.unigram_counts:
                    self.unigram_counts[token] = 0
                if not token in self.bigram_counts:
                    self.bigram_counts[token] = {}
                if not next_token in self.bigram_counts[token]:
                    self.bigram_counts[token][next_token] = 0
                self.unigram_counts[token] += 1
                self.bigram_counts[token][next_token] += 1

    def GenerateSample(self):
        sample = [start_token]
        while not sample[-1] == end_token:
            selector = random.uniform(0, self.unigram_counts[sample[-1]])
            sum_bc = 0
            for bigram in self.bigram_counts[sample[-1]]:
                sum_bc += self.bigram_counts[sample[-1]][bigram]
                if sum_bc > selector:
                    sample.append(bigram)
                    break
        return ''.join(sample[1:-1])

    # gets the (unsmoothed) probability of a string given the bigramlm
    def StringLogProbability(self, string):
        preprocessed = Preprocess([string])[0]
        logprob = 0
        for i in xrange(1, len(preprocessed)):
            unigram = preprocessed[i - 1]
            bigram = preprocessed[i]
            if unigram in self.unigram_counts and unigram in self.bigram_counts and bigram in self.bigram_counts[unigram]:
                logprob += log(self.bigram_counts[unigram][bigram]) - log(self.unigram_counts[unigram])
            else:
                logprob = float('-inf')
        return logprob

    def ExpectedGuesses(self, string):
        logprob = self.StringLogProbability(string)
        try:
            expectation = Decimal(-logprob).exp()
            return expectation if not isnan(expectation) else float('inf')
        except:
            return float('inf')

    def Generator(self):
        while True:
            yield self.GenerateSample()

    def SimplePrunedGenerator(self):
        tries = set()
        while True:
            pwd = self.GenerateSample()
            if not pwd in tries:
                tries.update([pwd])
                yield pwd

\end{lstlisting}
\newpage
\subsection{Hidden Markov Model Implementation}
\begin{lstlisting}
from math import log, exp, log1p, isnan
import random
from memoize import memoize
import nltk
from decimal import *

class HMMLM:
    def __init__(self, training_corpus, num_states):
        sequences = [[(c, "") for c in pwd] for pwd in training_corpus]
        symbols = list(set([c for pwd in training_corpus for c in pwd]))
        states = range(num_states)
        trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(states=states, symbols=symbols)
        self.hmm = trainer.train_unsupervised(sequences)

    def Sample(self, range_start, range_end):
        pwd = self.hmm.random_sample(random.Random(), random.randint(range_start, range_end))
        pwd = "".join([e[0] for e in pwd])
        return pwd

    def StringProbability(self, pwd):
        return self.hmm.log_probability([(c, None) for c in pwd])

    def ExpectedGuesses(self, pwd):
        logprob = self.StringProbability(pwd)
        try:
            expectation = Decimal(-logprob).exp()
            return expectation if not isnan(expectation) else float('inf')
        except:
            return float('inf')

    def Generator(self):
        while True:
            pwd = self.hmm.random_sample(random.Random(), random.randint(4, 18))
            pwd = "".join([e[0] for e in pwd])
            yield pwd


\end{lstlisting}
%%\subsection{Context-Free Grammar Implementation}

\end{document}
